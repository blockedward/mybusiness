kafka_log_retention_hours: 168
kafka_offsets_topic_replication_factor: 3
kafka_transaction_state_log_replication_factor: 3
kafka_confluent_metrics_reporter_topic_replicas: 3

kafka:
  broker:
    user: cp-kafka
    group: confluent
    config_file: /etc/kafka/server.properties
    systemd_file: /usr/lib/systemd/system/confluent-kafka.service
    systemd_override: /etc/systemd/system/confluent-kafka.service.d
    service_name: confluent-kafka
    datadir:
      - /var/lib/kafka/data
    systemd:
      enabled: yes
      state: started
    environment:
      KAFKA_HEAP_OPTS: "-Xmx1g"
    config:
      group.initial.rebalance.delay.ms: 0
      log.retention.check.interval.ms: 300000
      log.retention.hours: "{{kafka_log_retention_hours}}"
      log.segment.bytes: 1073741824
      num.io.threads: 16
      num.network.threads: 8
      num.partitions: 1
      num.recovery.threads.per.data.dir: 2
      offsets.topic.replication.factor: "{{kafka_offsets_topic_replication_factor}}"
      socket.receive.buffer.bytes: 102400
      socket.request.max.bytes: 104857600
      socket.send.buffer.bytes: 102400
      transaction.state.log.min.isr: 2
      transaction.state.log.replication.factor: "{{kafka_transaction_state_log_replication_factor}}"
      zookeeper.connection.timeout.ms: 6000
      metric.reporters: io.confluent.metrics.reporter.ConfluentMetricsReporter
      confluent.metrics.reporter.bootstrap.servers: "{{inventory_hostname}}:{{broker.config.port}}"
      confluent.metrics.reporter.topic.replicas: "{{kafka_confluent_metrics_reporter_topic_replicas}}"
      ssl.endpoint.identification.algorithm: ""
      confluent.metrics.reporter.ssl.endpoint.identification.algorithm: ""
